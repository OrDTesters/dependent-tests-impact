\begin{abstract}

In unit testing each test should properly initialize and reset its execution environment
and/or any resources it will use to avoid affecting another test's execution.
Order-dependent tests (dependent tests, for short) are tests whose execution results
are affected by another tests' execution. One common way dependent tests are
revealed in software systems is when developers utilize regression testing
techniques that alters the order in which tests of a suite are executed.
By changing the order in which tests are executed, tests that may have been passing
may now fail. The tests may fail due to its reliance on another test to
setup particular resources or environment. 

This paper studies the effects of order-dependent tests on a particular
regression testing technique called test prioritization.
Test prioritization schedules test cases in an order that increases their
effectiveness in meeting some performance goal. Some examples of these performance goals include increasing the likelihood of revealing faults earlier and increasing the coverage of code at a faster rate. When using test prioritization to generate a new execution order for a test suite, how often are the results of the new order affected by dependent tests? To what extent is the impact of them and how should we augment these existing regression testing techniques to deal with dependent tests? By developing a program that will generate test prioritization execution orders for test suites and studying five real-world programs, we provide concrete evidence to the questions above. We show that dependent tests does affect test prioritization techniques by analyzing five real-world programs. In three of the five programs we analyzed, their test suites attained different results when we applied test prioritization techniques to generate unique test execution orders. We also show how to deal with dependent tests by describing an impending feature for the tool we created to generate test prioritization execution orders.


\end{abstract}

